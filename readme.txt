cag的思想在于：在小数据量场景，将所有知识先利用大模型生成其对应的kvcache，并将该kvcache存入离线文件中。在问答场景，输入问题，从离线文件加载所有知识上下文的kvcache，然后给到大模型，回答问题。

其创新点在于：事先将知识上下文利用大模型生成transformer中的注意力kv值，持久化到文件中。问答阶段，加载文件中知识上下文的注意力kv值，并将知识上下文覆盖的问题和该注意力kv值给到大模型，生成回答。

缺点：给大模型提供全量知识上下文，导致问题与上下文的相关性变低，回答效果变差，回答的响应时间变长。

cag时长变长的可能原因：结合知识注意力kv值和问题提示词生成回答时，采用的是model对象的__call__方法，经测试发现【文章数1，段落数1，问题数1，见213/log_1.txt】平均每生成1个token，耗时约0.26秒，300个token的回答，耗时达7.5秒以上。

经过测试发现【文章数10，段落数100，问题数1000，见log.txt】，cag生成的答案与数据集中的答案平均相似度0.2左右，效果一般。