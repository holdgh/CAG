cag的思想在于：在小数据量场景，将所有知识先利用大模型生成其对应的kvcache，并将该kvcache存入离线文件中。在问答场景，输入问题，从离线文件加载所有知识上下文的kvcache，然后给到大模型，回答问题。

其创新点在于：事先将知识上下文利用大模型生成transformer中的注意力kv值，持久化到文件中。问答阶段，加载文件中知识上下文的注意力kv值，并将知识上下文覆盖的问题和该注意力kv值给到大模型，生成回答。

缺点：给大模型提供全量知识上下文，导致问题与上下文的相关性变低，回答效果变差，回答的响应时间变长。